---
layout:     post
title:      "spark"
subtitle:   "spark"
date:       2018-11-19 19:00:00
header-img: "img/post-bg-2015.jpg"
catalog: true
tags:
    - spark
---

# pyspark

SparkSession

```python
from pyspark.sql import SparkSession, SQLContext
from pyspark.sql.types import *
import pyspark.sql.functions as f

spark = SparkSession\
    .builder\
    .appName("AppName")\
    .getOrCreate()
sc = spark.sparkContext
sqlconext = SQLContext(sc)
```

# 读取

## 读取csv文件
```python
#file_name = "hdfs:///user/
#file_name = "file:///home/

# method 1:
# input_schema = StructType()
# for col in column_names:
#     input_schema.add(StructField(col, StringType(), True))              
# method 2:
# input_schema = spark.read.schema("col0 INT, col2 DOUBLE")

df = spark.read.csv(file_name, schema=input_schema, sep='\t')
```

## 读取任意文件
将文件读取为rdd，然后转换为dataframe， 使用rdd的map将文件行解析为结构化的字段

```python
def read_log_df():
    site = "1"
    category = "FOCUS2"
    log_rdd = sc.textFile(rankserver_log_dir)
    log_rdd = log_rdd.map(lambda x: map_log_data(x, site, category))
    log_rdd = log_rdd.filter(lambda x: x != "-")
    log_df = spark.createDataFrame(log_rdd, schema=["request_id", "location"])
    return log_df
log_df = read_log_df()
```


# 选择列
```python
df = df[['clk', 'site', 'category', 'location', 'ctr']]
```

# 修改列
```python
df = df.withColumn('ctr', f.col('ctr')/1000000)

# 增加常量列
df = df.withColumn('constant', f.lit(10))
```

# 过滤行
```python
df = df.filter(
        (f.col('site') == '1') & 
        (f.col('category') == 'FOCUS2')
        )
```

# 排序
```
df = df.orderBy(["age", "name"], ascending=[0, 1])
http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy
```

# 分组聚合
```python
df = df.groupBy(['location']).agg(
        f.count('clk'),
        f.sum('clk'), 
        f.sum('ctr'), 
        f.sum('ctr')/f.sum('clk'))
```
- 使用pyspark.sql.functions里的聚合函数

- pandas_udf 分组数据会转化为pd.DataFrame， 需要注意内存是否放的下分组数据， pandas_udf设置的返回类型
需要与函数返回的pd.DataFrame类型一致
```python
import pandas as pd
from pyspark.sql.types import *
from pyspark.sql.functions import pandas_udf, PandasUDFType

uid_schema = StructType()
uid_schema.add(StructField("uid", LongType(), True))
uid_schema.add(StructField("uid_finish_pv", LongType(), True))
uid_schema.add(StructField("uid_finish_clk", LongType(), True))
print("uid schema:", uid_schema)
@f.pandas_udf(uid_schema, f.PandasUDFType.GROUPED_MAP)
def uid_extract(pdf):
    d = {}
    d['uid'] = [pdf['uid'][0]]
    d['uid_finish_pv'] =  [len(pdf['finish'])]
    d['uid_finish_clk'] = [sum(pdf['finish'])]
    df = pd.DataFrame(d, columns=uid_schema.fieldNames())
    return df
uid_df = data_df.groupby(['uid']).apply(uid_extract)
```

# join
```python
left_df.join(right_df, ["request_id", "location"])
```

# RDD 和 DataFrame 转换
```
from pyspark.sql import SparkSession, SQLContext
from pyspark.sql.types import *

spark = SparkSession\
    .builder\
    .appName("StatLocationPctr")\
    .getOrCreate()
sc = spark.sparkContext
sqlContext = SQLContext(sc)

# DataFrame to RDD
df.rdd 

# RDD to DataFrame
input_schema = StructType()
input_schema.add(StructField("feature", StringType(), True))
input_schema.add(StructField("value", StringType(), True))
df = sqlContext.createDataFrame(rdd, input_schema)

df = sc.parallelize([
    (1, "foo", 2.0, "2016-02-16"),
    (2, "bar", 3.0, "2016-02-16")
]).toDF(["id", "x", "y", "date"])
```


# 保存rdd到文件
```
df.coalesce(1).write.save(output_dir, format="csv", sep='\t')
df.repartition(1).write.save(output_dir, format="csv", sep='\t')
df.rdd.saveAsTextFile(output_dir)
rdd.saveAsTextFile(output_dir)
rdd.repartition(1).saveAsTextFile(output_dir) 

# 按分区存储文件
df.write.partitionBy('year', 'month').save(dict_output_dir, format="csv", sep='\t')
```
需要特定格式的输出时， 可以使用map方法先拼接成特定格式的字符串，然后再输出

在保存df之前，如果对df进行了排序， repartition会打乱顺序， coalesce不会


# pyspark python


export SPARK_HOME=~/spark-2.4.0-bin-hadoop2.7
或者在~/spark-2.4.0-bin-hadoop2.7/conf/spark-env.sh 中配置SPARK_HOME， 后者会覆盖前者

export PATH=~/bin:~/hadoop/bin:~/spark-2.4.0-bin-hadoop2.7/bin:$PATH
设置path， 可以找到spark-submit


设置python
通过环境变量设置
export PYSPARK_PYTHON=/usr/local/bin/python2.7
export PYSPARK_DRIVER_PYTHON=/usr/local/bin/python2.7
export SPARK_YARN_USER_ENV="PYSPARK_PYTHON=/usr/local/bin/python2.7"

提交程序时设置python
    spark-submit \
        --master local \
        --conf "spark.pyspark.python=/home/appops/Python/bin/python" \
        --conf "spark.pyspark.driver.python=/home/appops/Python/bin/python" \


jar包
~/spark-2.4.0-bin-hadoop2.7/jars/
hadoop-lzo-0.4.20.jar

# spark streaming

## Discretized Stream (DStream)
```
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext("local[2]", "NetworkWordCount")
ssc = StreamingContext(sc, 1)

lines = ssc.socketTextStream("localhost", 9999)

```

##
flatMap


# Structured Streaming

```
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode
from pyspark.sql.functions import split

spark = SparkSession \
    .builder \
    .appName("StructuredNetworkWordCount") \
    .getOrCreate()
```

## 数据源

socket 读取数据
```python
lines = spark \
   .readStream \
   .format("socket") \
   .option("host", "localhost") \
   .option("port", 9993) \
   .load()
```

file 读取数据
```python
lines = spark \
    .readStream \
    .format("text") \
    .load("file:///home/<path>/hello*.txt")
```
- 启动时会处理所有匹配的文件
- 运行过程中删除文件，不会影响数据
- 运行过程中不会处理被更新的旧文件

## join


## 输出

file
```python
# format can be "orc", "json", "csv", etc.
query = spark.writeStream
    .format("parquet")     
    .option("path", "path/to/destination/dir")
    .start()
```

kafka
```python
query = spark.writeStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
    .option("topic", "updates")
    .start()
```